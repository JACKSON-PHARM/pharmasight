"""
Excel Import API endpoint with background job processing
"""
import logging
import threading
from fastapi import APIRouter, Depends, HTTPException, status, UploadFile, File, Form
from sqlalchemy.orm import Session
from typing import Optional
from uuid import UUID
import pandas as pd
from io import BytesIO
from datetime import datetime, timezone
import hashlib

from app.database import get_db
from app.services.excel_import_service import ExcelImportService
from app.models import ImportJob

logger = logging.getLogger(__name__)
router = APIRouter()


def process_import_job(
    job_id: UUID,
    company_id: UUID,
    branch_id: UUID,
    user_id: UUID,
    excel_data: list,
    force_mode: Optional[str]
):
    """
    Background task to process Excel import.
    Updates ImportJob progress as it processes.
    """
    from app.database import SessionLocal
    
    logger.info(f"üöÄ Background task STARTED for job {job_id} - Processing {len(excel_data)} rows")
    
    db = SessionLocal()
    try:
        # Get job record
        job = db.query(ImportJob).filter(ImportJob.id == job_id).first()
        if not job:
            logger.error(f"‚ùå Import job {job_id} not found in database")
            return
        
        logger.info(f"‚úÖ Found job {job_id}, status: {job.status}, total_rows: {job.total_rows}")
        
        # Update status to processing
        job.status = "processing"
        job.started_at = datetime.now(timezone.utc)
        db.commit()
        logger.info(f"üìä Job {job_id} status updated to 'processing'")
        
        # Import with progress tracking
        logger.info(f"üîÑ Starting Excel import for job {job_id}...")
        result = ExcelImportService.import_excel_data(
            db=db,
            company_id=company_id,
            branch_id=branch_id,
            user_id=user_id,
            excel_data=excel_data,
            force_mode=force_mode,
            job_id=job_id  # Pass job_id for progress updates
        )
        
        logger.info(f"‚úÖ Import completed for job {job_id}, result: {result}")
        
        # Refresh job to get latest state
        db.refresh(job)
        
        # Update job with results
        job.status = "completed"
        job.completed_at = datetime.now(timezone.utc)
        job.processed_rows = job.total_rows
        job.stats = result.get('stats', {})
        db.commit()
        
        logger.info(f"üéâ Import job {job_id} completed successfully - {job.processed_rows}/{job.total_rows} rows processed")
        
    except Exception as e:
        logger.error(f"‚ùå Import job {job_id} failed: {str(e)}", exc_info=True)
        import traceback
        error_traceback = traceback.format_exc()
        logger.error(f"Full traceback for job {job_id}:\n{error_traceback}")
        try:
            # Refresh job
            db.refresh(job) if 'job' in locals() else None
            job = db.query(ImportJob).filter(ImportJob.id == job_id).first()
            if job:
                job.status = "failed"
                job.error_message = str(e)[:1000]  # Limit error message length
                job.completed_at = datetime.now(timezone.utc)
                db.commit()
                logger.error(f"‚ùå Job {job_id} marked as failed in database")
        except Exception as db_error:
            logger.error(f"‚ùå Failed to update job status in database: {db_error}")
    finally:
        db.close()
        logger.info(f"üîí Database session closed for job {job_id}")


@router.post("/import")
async def import_excel(
    company_id: UUID = Form(...),
    branch_id: UUID = Form(...),
    user_id: UUID = Form(...),
    file: UploadFile = File(...),
    force_mode: Optional[str] = Form(None),
    db: Session = Depends(get_db)
):
    """
    Start Excel import as background job.
    Returns job_id immediately. Use GET /api/excel/import/{job_id}/progress to track progress.
    
    OPTIMIZED for production:
    - Non-blocking API response (< 1 second)
    - Background processing with progress tracking
    - Uses bulk operations (50-100x faster)
    - Handles network/power failures gracefully
    
    Two modes:
    - AUTHORITATIVE: Delete and recreate (only if no live transactions)
    - NON_DESTRUCTIVE: Create missing data only (when live transactions exist)
    """
    try:
        # Read Excel file
        contents = await file.read()
        
        # Calculate file hash for duplicate detection
        file_hash = hashlib.md5(contents).hexdigest()
        logger.info(f"Importing file with hash: {file_hash[:8]}...")
        
        # Check for duplicate import in progress
        existing_job = db.query(ImportJob).filter(
            ImportJob.company_id == company_id,
            ImportJob.file_hash == file_hash,
            ImportJob.status.in_(["pending", "processing"])
        ).first()
        
        if existing_job:
            return {
                'success': False,
                'message': 'Import already in progress',
                'job_id': str(existing_job.id),
                'status': existing_job.status,
                'progress_percent': (existing_job.processed_rows / existing_job.total_rows * 100) if existing_job.total_rows > 0 else 0
            }
        
        # Parse Excel
        df = pd.read_excel(BytesIO(contents))
        excel_data = df.to_dict('records')
        
        # Create import job record
        job = ImportJob(
            company_id=company_id,
            branch_id=branch_id,
            user_id=user_id,
            file_hash=file_hash,
            file_name=file.filename,
            status="pending",
            total_rows=len(excel_data),
            processed_rows=0
        )
        db.add(job)
        db.commit()
        db.refresh(job)
        
        # Start background task in a separate thread (more reliable than FastAPI BackgroundTasks)
        logger.info(f"üì§ Starting background thread for job {job.id} with {len(excel_data)} rows")
        try:
            thread = threading.Thread(
                target=process_import_job,
                args=(job.id, company_id, branch_id, user_id, excel_data, force_mode),
                daemon=False,  # Don't kill thread when main process exits
                name=f"ImportJob-{job.id}"
            )
            thread.start()
            logger.info(f"‚úÖ Background thread started for job {job.id} (thread ID: {thread.ident}) - API returning immediately")
        except Exception as thread_error:
            logger.error(f"‚ùå Failed to start background thread for job {job.id}: {thread_error}", exc_info=True)
            # Mark job as failed
            job.status = "failed"
            job.error_message = f"Failed to start background task: {str(thread_error)}"
            db.commit()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to start import: {str(thread_error)}"
            )
        
        return {
            'success': True,
            'message': 'Import started in background',
            'job_id': str(job.id),
            'total_rows': len(excel_data),
            'status': 'pending'
        }
        
    except ValueError as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except Exception as e:
        logger.error(f"Excel import error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Import failed: {str(e)}"
        )


@router.get("/import/{job_id}/progress")
def get_import_progress(job_id: UUID, db: Session = Depends(get_db)):
    """
    Get progress of an import job.
    Returns current status, progress percentage, and statistics.
    """
    job = db.query(ImportJob).filter(ImportJob.id == job_id).first()
    
    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Import job not found"
        )
    
    return job.to_dict()


@router.get("/mode/{company_id}")
def get_import_mode(company_id: UUID, db: Session = Depends(get_db)):
    """
    Get the current import mode for a company.
    
    Returns:
        - mode: 'AUTHORITATIVE' or 'NON_DESTRUCTIVE'
        - has_live_transactions: bool
    """
    has_live = ExcelImportService.has_live_transactions(db, company_id)
    mode = ExcelImportService.detect_import_mode(db, company_id)
    
    return {
        'mode': mode,
        'has_live_transactions': has_live,
        'message': 'AUTHORITATIVE mode allows reset. NON_DESTRUCTIVE mode preserves existing data.'
    }
